{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update sklearn to prevent version mismatches\n",
    "# !pip install sklearn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install joblib. This will be used to save your model. \n",
    "# Restart your kernel after installing \n",
    "# !pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the CSV and Perform Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>koi_fpflag_nt</th>\n",
       "      <th>koi_fpflag_ss</th>\n",
       "      <th>koi_fpflag_co</th>\n",
       "      <th>koi_fpflag_ec</th>\n",
       "      <th>koi_period</th>\n",
       "      <th>koi_period_err1</th>\n",
       "      <th>koi_period_err2</th>\n",
       "      <th>koi_time0bk</th>\n",
       "      <th>koi_time0bk_err1</th>\n",
       "      <th>koi_time0bk_err2</th>\n",
       "      <th>...</th>\n",
       "      <th>koi_steff_err2</th>\n",
       "      <th>koi_slogg</th>\n",
       "      <th>koi_slogg_err1</th>\n",
       "      <th>koi_slogg_err2</th>\n",
       "      <th>koi_srad</th>\n",
       "      <th>koi_srad_err1</th>\n",
       "      <th>koi_srad_err2</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>koi_kepmag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6.991000e+03</td>\n",
       "      <td>6.991000e+03</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.00000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "      <td>6991.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.157059</td>\n",
       "      <td>0.244743</td>\n",
       "      <td>0.202975</td>\n",
       "      <td>0.125018</td>\n",
       "      <td>56.191248</td>\n",
       "      <td>1.851122e-03</td>\n",
       "      <td>-1.851122e-03</td>\n",
       "      <td>164.488820</td>\n",
       "      <td>0.009340</td>\n",
       "      <td>-0.009340</td>\n",
       "      <td>...</td>\n",
       "      <td>-161.206980</td>\n",
       "      <td>4.305049</td>\n",
       "      <td>0.121091</td>\n",
       "      <td>-0.14048</td>\n",
       "      <td>1.740749</td>\n",
       "      <td>0.352710</td>\n",
       "      <td>-0.388568</td>\n",
       "      <td>292.082406</td>\n",
       "      <td>43.812143</td>\n",
       "      <td>14.271508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.363882</td>\n",
       "      <td>0.429966</td>\n",
       "      <td>0.402243</td>\n",
       "      <td>0.330763</td>\n",
       "      <td>117.570962</td>\n",
       "      <td>7.184503e-03</td>\n",
       "      <td>7.184503e-03</td>\n",
       "      <td>67.020475</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>0.021989</td>\n",
       "      <td>...</td>\n",
       "      <td>71.448481</td>\n",
       "      <td>0.439238</td>\n",
       "      <td>0.132048</td>\n",
       "      <td>0.08199</td>\n",
       "      <td>5.903415</td>\n",
       "      <td>0.839017</td>\n",
       "      <td>1.907797</td>\n",
       "      <td>4.762908</td>\n",
       "      <td>3.606167</td>\n",
       "      <td>1.350802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259820</td>\n",
       "      <td>1.100000e-08</td>\n",
       "      <td>-1.568000e-01</td>\n",
       "      <td>120.515914</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>-0.569000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1733.000000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.00700</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-103.825000</td>\n",
       "      <td>279.856080</td>\n",
       "      <td>36.577381</td>\n",
       "      <td>6.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.620126</td>\n",
       "      <td>5.005000e-06</td>\n",
       "      <td>-2.401000e-04</td>\n",
       "      <td>132.683917</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>-197.000000</td>\n",
       "      <td>4.209000</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>-0.19500</td>\n",
       "      <td>0.829000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>-0.252000</td>\n",
       "      <td>288.704730</td>\n",
       "      <td>40.797760</td>\n",
       "      <td>13.455000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.947426</td>\n",
       "      <td>3.300000e-05</td>\n",
       "      <td>-3.300000e-05</td>\n",
       "      <td>136.739230</td>\n",
       "      <td>0.003990</td>\n",
       "      <td>-0.003990</td>\n",
       "      <td>...</td>\n",
       "      <td>-159.000000</td>\n",
       "      <td>4.436000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-0.12700</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>-0.111000</td>\n",
       "      <td>292.314760</td>\n",
       "      <td>43.679661</td>\n",
       "      <td>14.534000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.282605</td>\n",
       "      <td>2.401000e-04</td>\n",
       "      <td>-5.005000e-06</td>\n",
       "      <td>169.937005</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-0.001145</td>\n",
       "      <td>...</td>\n",
       "      <td>-112.000000</td>\n",
       "      <td>4.543000</td>\n",
       "      <td>0.149000</td>\n",
       "      <td>-0.08800</td>\n",
       "      <td>1.357000</td>\n",
       "      <td>0.357000</td>\n",
       "      <td>-0.069000</td>\n",
       "      <td>295.888550</td>\n",
       "      <td>46.693659</td>\n",
       "      <td>15.322000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1071.232624</td>\n",
       "      <td>1.568000e-01</td>\n",
       "      <td>-1.100000e-08</td>\n",
       "      <td>1472.522306</td>\n",
       "      <td>0.569000</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.364000</td>\n",
       "      <td>1.472000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>180.013000</td>\n",
       "      <td>25.956000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>301.720760</td>\n",
       "      <td>52.336010</td>\n",
       "      <td>19.065000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  koi_fpflag_ec  \\\n",
       "count    6991.000000    6991.000000    6991.000000    6991.000000   \n",
       "mean        0.157059       0.244743       0.202975       0.125018   \n",
       "std         0.363882       0.429966       0.402243       0.330763   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "        koi_period  koi_period_err1  koi_period_err2  koi_time0bk  \\\n",
       "count  6991.000000     6.991000e+03     6.991000e+03  6991.000000   \n",
       "mean     56.191248     1.851122e-03    -1.851122e-03   164.488820   \n",
       "std     117.570962     7.184503e-03     7.184503e-03    67.020475   \n",
       "min       0.259820     1.100000e-08    -1.568000e-01   120.515914   \n",
       "25%       2.620126     5.005000e-06    -2.401000e-04   132.683917   \n",
       "50%       8.947426     3.300000e-05    -3.300000e-05   136.739230   \n",
       "75%      34.282605     2.401000e-04    -5.005000e-06   169.937005   \n",
       "max    1071.232624     1.568000e-01    -1.100000e-08  1472.522306   \n",
       "\n",
       "       koi_time0bk_err1  koi_time0bk_err2  ...  koi_steff_err2    koi_slogg  \\\n",
       "count       6991.000000       6991.000000  ...     6991.000000  6991.000000   \n",
       "mean           0.009340         -0.009340  ...     -161.206980     4.305049   \n",
       "std            0.021989          0.021989  ...       71.448481     0.439238   \n",
       "min            0.000009         -0.569000  ...    -1733.000000     0.047000   \n",
       "25%            0.001145         -0.010000  ...     -197.000000     4.209000   \n",
       "50%            0.003990         -0.003990  ...     -159.000000     4.436000   \n",
       "75%            0.010000         -0.001145  ...     -112.000000     4.543000   \n",
       "max            0.569000         -0.000009  ...        0.000000     5.364000   \n",
       "\n",
       "       koi_slogg_err1  koi_slogg_err2     koi_srad  koi_srad_err1  \\\n",
       "count     6991.000000      6991.00000  6991.000000    6991.000000   \n",
       "mean         0.121091        -0.14048     1.740749       0.352710   \n",
       "std          0.132048         0.08199     5.903415       0.839017   \n",
       "min          0.000000        -1.00700     0.109000       0.000000   \n",
       "25%          0.044000        -0.19500     0.829000       0.128000   \n",
       "50%          0.070000        -0.12700     0.999000       0.248000   \n",
       "75%          0.149000        -0.08800     1.357000       0.357000   \n",
       "max          1.472000         0.00000   180.013000      25.956000   \n",
       "\n",
       "       koi_srad_err2           ra          dec   koi_kepmag  \n",
       "count    6991.000000  6991.000000  6991.000000  6991.000000  \n",
       "mean       -0.388568   292.082406    43.812143    14.271508  \n",
       "std         1.907797     4.762908     3.606167     1.350802  \n",
       "min      -103.825000   279.856080    36.577381     6.966000  \n",
       "25%        -0.252000   288.704730    40.797760    13.455000  \n",
       "50%        -0.111000   292.314760    43.679661    14.534000  \n",
       "75%        -0.069000   295.888550    46.693659    15.322000  \n",
       "max         0.000000   301.720760    52.336010    19.065000  \n",
       "\n",
       "[8 rows x 40 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"exoplanet_data.csv\")\n",
    "# Drop the null columns where all values are null\n",
    "df = df.dropna(axis='columns', how='all')\n",
    "# Drop the null rows\n",
    "df = df.dropna()  # empty values, \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            CONFIRMED\n",
      "1       FALSE POSITIVE\n",
      "2       FALSE POSITIVE\n",
      "3            CONFIRMED\n",
      "4            CONFIRMED\n",
      "             ...      \n",
      "6986    FALSE POSITIVE\n",
      "6987    FALSE POSITIVE\n",
      "6988         CANDIDATE\n",
      "6989    FALSE POSITIVE\n",
      "6990    FALSE POSITIVE\n",
      "Name: koi_disposition, Length: 6991, dtype: object\n",
      "[0 1 1 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# X = df.drop('koi_disposition', axis=1).values.reshape(6991, 40)\n",
    "X = df.drop(['koi_disposition','koi_srad','koi_slogg', 'koi_insol_err2','koi_slogg_err1'\n",
    "            ,'koi_prad_err2','koi_tce_plnt_num','koi_teq','koi_insol','koi_impact_err2'\n",
    "            ,'koi_depth','koi_srad_err2','koi_kepmag','koi_period_err1','koi_duration_err1'\n",
    "            , 'koi_time0bk_err1', 'koi_period_err2','koi_steff', 'koi_prad_err1', 'koi_impact_err1'\n",
    "            , 'koi_depth_err2', 'koi_slogg_err2', 'koi_depth_err1', 'koi_duration_err2', 'koi_duration'\n",
    "            , \n",
    "             ], axis = 1)   # 'koi_steff_err1', 'koi_time0bk_err2', 'koi_period', 'koi_srad_err1','dec'\n",
    "X = X.values.reshape(X.shape[0],X.shape[1])\n",
    "y = df['koi_disposition']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(y)\n",
    "\n",
    "# Label encode data set to 0, 1 or 2\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "y = label_encoder.transform(y)\n",
    "# label_encoder.classes_ # the variables\n",
    "\n",
    "# Convert the target from 3 variables to 2 variables by combining Confirmed and Candidate\n",
    "y = np.where(y== 1, 0, y)\n",
    "y = np.where(y== 2,1, y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.utils import to_categorical\n",
    "# one_hot_y = to_categorical(y)\n",
    "# one_hot_y   # for Neuro network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select your features (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features. This will also be used as your x values.\n",
    "# selected_features = df[['names', 'of', 'selected', 'features', 'here']] # used to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Train Test Split\n",
    "\n",
    "Use `koi_disposition` for the y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# X = pd.get_dummies(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Scale the data using the MinMaxScaler and perform some feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale your data  May or may not affect the accuracy of the model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.9897005531184436\n",
      "Testing Data Score: 0.9891304347826086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=1000)   #(max_iter=100)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# print(model.score(X_train_scaled, y_train), model.score(X_test_scaled,y_test))\n",
    "\n",
    "# predictions = model.predict(X_test)  # value of each\n",
    "# print(f\"First 10 Predictions:)   {predictions[:10]}\")\n",
    "# print(f\"First 10 Actual Labels: {y_test[:10].tolist()}\")\n",
    "\n",
    "# predictions = model.predict_proba(X_test)  # probablity of each\n",
    "# print(f\"First 10 Predictions:)   {predictions[:10]}\")\n",
    "# print(f\"First 10 Actual Labels: {y_test[:10].tolist()}\")\n",
    "\n",
    "\n",
    "print(f\"Training Data Score: {model.score(X_train_scaled, y_train)}\")\n",
    "print(f\"Testing Data Score: {model.score(X_test_scaled, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "Use `GridSearchCV` to tune the model's parameters\n",
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "sklearn Logisics: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 1000, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# Create the GridSearchCV model   \n",
    "from sklearn.model_selection import GridSearchCV  # algorythm based on light look for best possible accuracy\n",
    "param_grid = {\"C\": [0.01, 0.1, 1, 10, 100, 150],    # adjustments, Note that regularization is applied by default.\n",
    "             'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "             'solver':['liblinear'],\n",
    "             'max_iter':[1200]}\n",
    "        \n",
    "model = LogisticRegression(max_iter=1000)  # solver='liblinear'\n",
    "print(model.get_params())\n",
    "grid = GridSearchCV(model, param_grid, verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV] C=0.01, max_iter=1200, penalty=l1, solver=liblinear .............\n",
      "[CV]  C=0.01, max_iter=1200, penalty=l1, solver=liblinear, score=0.985, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=l1, solver=liblinear .............\n",
      "[CV]  C=0.01, max_iter=1200, penalty=l1, solver=liblinear, score=0.972, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=l1, solver=liblinear .............\n",
      "[CV]  C=0.01, max_iter=1200, penalty=l1, solver=liblinear, score=0.982, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=l1, solver=liblinear .............\n",
      "[CV]  C=0.01, max_iter=1200, penalty=l1, solver=liblinear, score=0.972, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=l1, solver=liblinear .............\n",
      "[CV]  C=0.01, max_iter=1200, penalty=l1, solver=liblinear, score=0.980, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=l2, solver=liblinear .............\n",
      "[CV]  C=0.01, max_iter=1200, penalty=l2, solver=liblinear, score=0.985, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=l2, solver=liblinear .............\n",
      "[CV]  C=0.01, max_iter=1200, penalty=l2, solver=liblinear, score=0.972, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=l2, solver=liblinear .............\n",
      "[CV]  C=0.01, max_iter=1200, penalty=l2, solver=liblinear, score=0.982, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=l2, solver=liblinear .............\n",
      "[CV]  C=0.01, max_iter=1200, penalty=l2, solver=liblinear, score=0.972, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=l2, solver=liblinear .............\n",
      "[CV]  C=0.01, max_iter=1200, penalty=l2, solver=liblinear, score=0.980, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=elasticnet, solver=liblinear .....\n",
      "[CV]  C=0.01, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=elasticnet, solver=liblinear .....\n",
      "[CV]  C=0.01, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=elasticnet, solver=liblinear .....\n",
      "[CV]  C=0.01, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=elasticnet, solver=liblinear .....\n",
      "[CV]  C=0.01, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=elasticnet, solver=liblinear .....\n",
      "[CV]  C=0.01, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=none, solver=liblinear ...........\n",
      "[CV]  C=0.01, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=none, solver=liblinear ...........\n",
      "[CV]  C=0.01, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=none, solver=liblinear ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=0.01, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=none, solver=liblinear ...........\n",
      "[CV]  C=0.01, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.01, max_iter=1200, penalty=none, solver=liblinear ...........\n",
      "[CV]  C=0.01, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=l1, solver=liblinear, score=0.995, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=l1, solver=liblinear, score=0.989, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=l1, solver=liblinear, score=0.988, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=l1, solver=liblinear, score=0.984, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=l1, solver=liblinear, score=0.993, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=l2, solver=liblinear, score=0.993, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=l2, solver=liblinear, score=0.987, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=l2, solver=liblinear, score=0.987, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=l2, solver=liblinear, score=0.983, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=l2, solver=liblinear, score=0.991, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=0.1, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=0.1, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=0.1, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=0.1, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=0.1, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=0.1, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=0.1, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=l1, solver=liblinear ................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=1, max_iter=1200, penalty=l1, solver=liblinear, score=0.995, total=   0.1s\n",
      "[CV] C=1, max_iter=1200, penalty=l1, solver=liblinear ................\n",
      "[CV]  C=1, max_iter=1200, penalty=l1, solver=liblinear, score=0.989, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=l1, solver=liblinear ................\n",
      "[CV]  C=1, max_iter=1200, penalty=l1, solver=liblinear, score=0.988, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=l1, solver=liblinear ................\n",
      "[CV]  C=1, max_iter=1200, penalty=l1, solver=liblinear, score=0.984, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=l1, solver=liblinear ................\n",
      "[CV]  C=1, max_iter=1200, penalty=l1, solver=liblinear, score=0.993, total=   0.1s\n",
      "[CV] C=1, max_iter=1200, penalty=l2, solver=liblinear ................\n",
      "[CV]  C=1, max_iter=1200, penalty=l2, solver=liblinear, score=0.995, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=l2, solver=liblinear ................\n",
      "[CV]  C=1, max_iter=1200, penalty=l2, solver=liblinear, score=0.989, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=l2, solver=liblinear ................\n",
      "[CV]  C=1, max_iter=1200, penalty=l2, solver=liblinear, score=0.988, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=l2, solver=liblinear ................\n",
      "[CV]  C=1, max_iter=1200, penalty=l2, solver=liblinear, score=0.984, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=l2, solver=liblinear ................\n",
      "[CV]  C=1, max_iter=1200, penalty=l2, solver=liblinear, score=0.993, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=elasticnet, solver=liblinear ........\n",
      "[CV]  C=1, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=elasticnet, solver=liblinear ........\n",
      "[CV]  C=1, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=elasticnet, solver=liblinear ........\n",
      "[CV]  C=1, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=elasticnet, solver=liblinear ........\n",
      "[CV]  C=1, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=elasticnet, solver=liblinear ........\n",
      "[CV]  C=1, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=none, solver=liblinear ..............\n",
      "[CV]  C=1, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=none, solver=liblinear ..............\n",
      "[CV]  C=1, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=none, solver=liblinear ..............\n",
      "[CV]  C=1, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=none, solver=liblinear ..............\n",
      "[CV]  C=1, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=1, max_iter=1200, penalty=none, solver=liblinear ..............\n",
      "[CV]  C=1, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=l1, solver=liblinear ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=10, max_iter=1200, penalty=l1, solver=liblinear, score=0.995, total=   0.2s\n",
      "[CV] C=10, max_iter=1200, penalty=l1, solver=liblinear ...............\n",
      "[CV]  C=10, max_iter=1200, penalty=l1, solver=liblinear, score=0.989, total=   0.1s\n",
      "[CV] C=10, max_iter=1200, penalty=l1, solver=liblinear ...............\n",
      "[CV]  C=10, max_iter=1200, penalty=l1, solver=liblinear, score=0.988, total=   0.1s\n",
      "[CV] C=10, max_iter=1200, penalty=l1, solver=liblinear ...............\n",
      "[CV]  C=10, max_iter=1200, penalty=l1, solver=liblinear, score=0.984, total=   0.1s\n",
      "[CV] C=10, max_iter=1200, penalty=l1, solver=liblinear ...............\n",
      "[CV]  C=10, max_iter=1200, penalty=l1, solver=liblinear, score=0.993, total=   0.2s\n",
      "[CV] C=10, max_iter=1200, penalty=l2, solver=liblinear ...............\n",
      "[CV]  C=10, max_iter=1200, penalty=l2, solver=liblinear, score=0.995, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=l2, solver=liblinear ...............\n",
      "[CV]  C=10, max_iter=1200, penalty=l2, solver=liblinear, score=0.989, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=l2, solver=liblinear ...............\n",
      "[CV]  C=10, max_iter=1200, penalty=l2, solver=liblinear, score=0.988, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=l2, solver=liblinear ...............\n",
      "[CV]  C=10, max_iter=1200, penalty=l2, solver=liblinear, score=0.984, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=l2, solver=liblinear ...............\n",
      "[CV]  C=10, max_iter=1200, penalty=l2, solver=liblinear, score=0.993, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=elasticnet, solver=liblinear .......\n",
      "[CV]  C=10, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=elasticnet, solver=liblinear .......\n",
      "[CV]  C=10, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=elasticnet, solver=liblinear .......\n",
      "[CV]  C=10, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=elasticnet, solver=liblinear .......\n",
      "[CV]  C=10, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=elasticnet, solver=liblinear .......\n",
      "[CV]  C=10, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=none, solver=liblinear .............\n",
      "[CV]  C=10, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=none, solver=liblinear .............\n",
      "[CV]  C=10, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=none, solver=liblinear .............\n",
      "[CV]  C=10, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=none, solver=liblinear .............\n",
      "[CV]  C=10, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=10, max_iter=1200, penalty=none, solver=liblinear .............\n",
      "[CV]  C=10, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=l1, solver=liblinear ..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=100, max_iter=1200, penalty=l1, solver=liblinear, score=0.994, total=   0.2s\n",
      "[CV] C=100, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=100, max_iter=1200, penalty=l1, solver=liblinear, score=0.989, total=   0.5s\n",
      "[CV] C=100, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=100, max_iter=1200, penalty=l1, solver=liblinear, score=0.987, total=   0.2s\n",
      "[CV] C=100, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=100, max_iter=1200, penalty=l1, solver=liblinear, score=0.984, total=   0.5s\n",
      "[CV] C=100, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=100, max_iter=1200, penalty=l1, solver=liblinear, score=0.993, total=   0.1s\n",
      "[CV] C=100, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=100, max_iter=1200, penalty=l2, solver=liblinear, score=0.995, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=100, max_iter=1200, penalty=l2, solver=liblinear, score=0.989, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=100, max_iter=1200, penalty=l2, solver=liblinear, score=0.988, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=100, max_iter=1200, penalty=l2, solver=liblinear, score=0.984, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=100, max_iter=1200, penalty=l2, solver=liblinear, score=0.993, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=100, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=100, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=100, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=100, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=100, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=100, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=100, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=100, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=100, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=100, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=100, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=l1, solver=liblinear ..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  C=150, max_iter=1200, penalty=l1, solver=liblinear, score=0.994, total=   0.1s\n",
      "[CV] C=150, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=150, max_iter=1200, penalty=l1, solver=liblinear, score=0.989, total=   0.4s\n",
      "[CV] C=150, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=150, max_iter=1200, penalty=l1, solver=liblinear, score=0.987, total=   0.1s\n",
      "[CV] C=150, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=150, max_iter=1200, penalty=l1, solver=liblinear, score=0.984, total=   0.3s\n",
      "[CV] C=150, max_iter=1200, penalty=l1, solver=liblinear ..............\n",
      "[CV]  C=150, max_iter=1200, penalty=l1, solver=liblinear, score=0.993, total=   0.1s\n",
      "[CV] C=150, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=150, max_iter=1200, penalty=l2, solver=liblinear, score=0.995, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=150, max_iter=1200, penalty=l2, solver=liblinear, score=0.989, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=150, max_iter=1200, penalty=l2, solver=liblinear, score=0.987, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=150, max_iter=1200, penalty=l2, solver=liblinear, score=0.984, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=l2, solver=liblinear ..............\n",
      "[CV]  C=150, max_iter=1200, penalty=l2, solver=liblinear, score=0.993, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=150, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=150, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=150, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=150, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=elasticnet, solver=liblinear ......\n",
      "[CV]  C=150, max_iter=1200, penalty=elasticnet, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=150, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=150, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=150, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=150, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n",
      "[CV] C=150, max_iter=1200, penalty=none, solver=liblinear ............\n",
      "[CV]  C=150, max_iter=1200, penalty=none, solver=liblinear, score=nan, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 450, in _check_solver\n",
      "    \" got solver={}.\".format(solver))\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:552: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\paule\\anaconda3\\envs\\PythonData\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 454, in _check_solver\n",
      "    \"penalty='none' is not supported for the liblinear solver\"\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "  FitFailedWarning)\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(max_iter=1000),\n",
       "             param_grid={'C': [0.01, 0.1, 1, 10, 100, 150], 'max_iter': [1200],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
       "                         'solver': ['liblinear']},\n",
       "             verbose=3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with GridSearch\n",
    "#fit the model using the grid search extimator\n",
    "grid.fit(X_train_scaled, y_train)  # replaced grid with model2\n",
    "#List the best parameters for this dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'max_iter': 1200, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "0.9897001142491213\n"
     ]
    }
   ],
   "source": [
    "#Make predictions with the hpyertuned model\n",
    "predictions = grid.predict(X_test_scaled)\n",
    "\n",
    "#List the best score\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       895\n",
      "           1       0.98      1.00      0.99       853\n",
      "\n",
      "    accuracy                           0.99      1748\n",
      "   macro avg       0.99      0.99      0.99      1748\n",
      "weighted avg       0.99      0.99      0.99      1748\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions)) #, target_names=['']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model by updating \"your_name\" with your name\n",
    "# and \"your_model\" with your model variable\n",
    "# be sure to turn this in to BCS\n",
    "# if joblib fails to import, try running the command to install in terminal/git-bash\n",
    "# import joblib\n",
    "# filename = 'planet_logistic.sav'\n",
    "# joblib.dump(your_model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "dev"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
